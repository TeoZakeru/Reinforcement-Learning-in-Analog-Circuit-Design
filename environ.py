# import gymnasium as gym
# from gymnasium import spaces
# import numpy as np
# import subprocess
# import ltspice

# class W1W2OptimizationEnv(gym.Env):
#     """
#     A Gym environment for optimizing 24 discrete weights (w1..w24) for circuit performance.

#     Observation: indices of each weight in its discrete value list
#     Action: for each dimension, either increment or decrement its index
#     Reward: sum of all weights
#     """
#     def __init__(self):
#         super().__init__()

#         # Define discrete value sets for each weight (24 weights in total)
#         self.weight_values = [
#             list(range(1, 11)),        # w1: 1..10
#             list(range(1, 11)),    # w2: 30, 32, ..., 50
#             list(range(1, 11)),        # w3: 1..10
#             list(range(1, 11))    # w4
#             # list(range(1, 11)),        # w5
#             # list(range(30, 51, 2)),    # w6
#             # list(range(1, 11)),        # w7
#             # list(range(30, 51, 2)),    # w8
#             # list(range(1, 11)),        # w9
#             # list(range(30, 51, 2)),    # w10
#             # list(range(1, 11)),        # w11
#             # list(range(30, 51, 2)),    # w12
#             # list(range(1, 11)),        # w13
#             # list(range(30, 51, 2)),    # w14
#             # list(range(1, 11)),        # w15
#             # list(range(30, 51, 2)),    # w16
#             # list(range(1, 11)),        # w17
#             # list(range(30, 51, 2)),    # w18
#             # list(range(1, 11)),        # w19
#             # list(range(30, 51, 2)),    # w20
#             # list(range(1, 11)),        # w21
#             # list(range(30, 51, 2)),    # w22
#             # list(range(1, 11)),        # w23
#             # list(range(30, 51, 2)),    # w24
#         ]
#         self.num_dims = len(self.weight_values)

#         # Observation: a MultiDiscrete over the index in each weight_values list
#         self.observation_space = spaces.MultiDiscrete([
#             len(vals) for vals in self.weight_values
#         ])

#         # Action: 2 actions per dimension (increment or decrement)
#         self.action_space = spaces.Discrete(2 * self.num_dims)

#         # Start in the center of each dimension
#         self.state = tuple(len(vals) // 2 for vals in self.weight_values)
#         self.steps = 0
#         self.max_steps = 1_000

#         # Track best state for evaluation
#         self.best_reward = float('-inf')
#         self.best_state = self.state

#     def run_simulation(self,weights):
#     # Convert to SPICE-compatible width values
        

#     # üîß Netlist generation
#         netlist = f'''* C:\\Users\\Rohit\\Desktop\\RL\\ldo.asc
# * Generated by LTspice 24.1.6 for Windows.
# M1 OTAout Vref N003 N003 nmos292p L=l1 W={weights[0]}u
# M2 N001 N002 N003 N003 nmos292p L=l1 W={weights[1]}u
# M3 N003 Bias 0 0 nmos292p L=0.72u W=10u
# M4 N001 N001 Vdd Vdd pmos292p L=lload W={weights[2]}u
# M5 OTAout N001 Vdd Vdd pmos292p L=lload W={weights[3]}u
# MPass OUT OTAout Vdd Vdd pmos292p L=0.18u W=Wpass
# M7 OUT Bias 0 0 nmos292p L=0.72u W=10u
# V1 Vdd 0 1.4 AC 0
# V2 Vref 0 1.2
# M8 Bias Bias 0 0 nmos292p L=0.72u W=10u
# I1 Vdd Bias 50u
# Iload OUT 0 10m
# C2 OUT 0 1¬µ
# V3 N002 N004 0 AC 1
# R1 OUT N004 1G
# C3 N004 0 1
# .model NMOS NMOS
# .model PMOS PMOS
# .lib C:\\Users\\Rohit\\AppData\\Local\\LTspice\\lib\\cmp\\standard.mos
# .include models_292p.txt
# .param ibias=50u
# .param l1=0.72u
# .param lload=0.18u
# .tran 400u
# .param Wload=2.7u
# .param Wdiff=4.31u
# .param Wpass=1132u
# .param Cload=1u
# .backanno
# .end'''

#     # üìÅ File paths
#         netlist_path = r"C:\\Users\\Rohit\\Desktop\\RL\\RL_Project\\ldo.cir"
#         raw_file = r"C:\\Users\\Rohit\\Desktop\\RL\\RL_Project\\ldo.raw"
#         ltspice_path = r"C:\\Users\\Rohit\\AppData\\Local\\Programs\\ADI\\LTspice\\LTspice.exe"

#     # Save netlist
#         with open(netlist_path, "w") as f:
#             f.write(netlist)

#         print(f"Netlist saved at: {netlist_path}")

#     # Run LTspice
#         subprocess.run([ltspice_path, "-b", netlist_path], check=True)
#         print("Simulation completed.")

#     # üìà Load simulation results
#         l = ltspice.Ltspice(raw_file)
#         l.parse()

#         time = l.get_time()
#         vdd = np.max(l.get_data('V(vdd)'))
#         vref = l.get_data('V(vref)')
#         vout = l.get_data('V(out)')
#         i_vdd = np.abs(np.min(l.get_data('I(V1)')))

#     # üîç Calculate Error and Power
#         error = np.sum(np.abs(vout - vref))
#         power = i_vdd*vdd
#         return power, error

#     def reset(self, seed=None, options=None):
#         super().reset(seed=seed)
#         # Random initialization for better exploration
#         self.state = tuple(np.random.randint(0, len(vals)) for vals in self.weight_values)
#         self.steps = 0
#         return np.array(self.state, dtype=np.int32), {}

#     def step(self, action):
#         # Decode action: which dimension and which direction
#         idxs = list(self.state)
#         dim = action // 2
#         increment = (action % 2) == 0

#         # Update index within bounds
#         if increment and idxs[dim] < len(self.weight_values[dim]) - 1:
#             idxs[dim] += 1
#         elif not increment and idxs[dim] > 0:
#             idxs[dim] -= 1

#         self.state = tuple(idxs)
#         self.steps += 1

#         # Map indices to actual weight values
#         weights = [self.weight_values[i][idx] for i, idx in enumerate(self.state)]

#         power, error = self.run_simulation(weights)
#         # New reward: 1/w1 + (w2)^2
#         reward = float(1/power + error*1e6)

#         done = (self.steps >= self.max_steps)
#         return np.array(self.state, dtype=np.int32), reward, done, False, {}

#         # Calculate reward
#         #reward = self.calculate_reward(weights)
#         # target_weights = [5, 40, 5, 40, 5, 40, 5, 40, 5, 40, 5, 40, 5, 40, 5, 40, 5, 40, 5, 40, 5, 40, 5, 40]
#         # squared_errors = [(weights[i] - target_weights[i]) ** 2 for i in range(24)]
#         # reward = -sum(squared_errors)

#         # # Track best state
#         # if reward > self.best_reward:
#         #     self.best_reward = reward
#         #     self.best_state = self.state

#         # done = self.steps >= self.max_steps

#         # # Add small Gaussian noise to promote exploration
#         # reward += np.random.normal(0, 0.1)

#         # return np.array(self.state, dtype=np.int32), reward, done, False, {}

#     def render(self):
#         weights = {f"w{i+1}": self.weight_values[i][idx]
#                    for i, idx in enumerate(self.state)}
#         state_str = ", ".join(f"{k}={v}" for k, v in weights.items())
#         print(f"State: {state_str}")

#         best_weights = {f"w{i+1}": self.weight_values[i][idx]
#                         for i, idx in enumerate(self.best_state)}
#         best_str = ", ".join(f"{k}={v}" for k, v in best_weights.items())
#         print(f"Best state found: {best_str} (reward: {self.best_reward:.4f})")

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import subprocess
import ltspice
from openpyxl import load_workbook
import pandas as pd


df = pd.read_csv(r"C:\\Users\\Rohit\\Desktop\\RL\\RL_Project\\merged_simulation_results_0_159999_full.csv")

# lookup = {
#     (int(r.w1),int(r.w2),int(r.w3),int(r.w4)): (r["Power (W)"], r["Error (V)"])
#     for _, r in df.iterrows()
# }
lookup = {
    (tuple(map(int, r.widths.strip("()").split(", "))),
     tuple(map(int, r.configs.strip("()").split(", ")))): (r.power, r.power1, r.error)
    for _, r in df.iterrows()
}

# count = 0
# for (widths, config), (power, power1, error) in lookup.items():
#     count += 1
#     print(f"Widths: {widths}, Config: {config}, Power: {power}, Power1: {power1}, Error: {error}")
# print(count)

# for i in lookup:
#     print(i)


class W1W2OptimizationEnv(gym.Env):
    def __init__(self):
        super().__init__()

        # Define discrete value sets for each weight (24 weights in total)
        self.weight_values = [
            list(range(1, 11)),        # w1: 1..10
            list(range(1, 11)),    # w2: 30, 32, ..., 50
            list(range(1, 11)),        # w3: 1..10
            list(range(1, 11)),    # w4
            list(range(1, 3)),
            list(range(1, 3)),
            list(range(1, 3)),
            list(range(1, 3))
        ]
        self.num_dims = len(self.weight_values)

        # Observation: a MultiDiscrete over the index in each weight_values list
        self.observation_space = spaces.MultiDiscrete([
            len(vals) for vals in self.weight_values
        ])

        # Action: 2 actions per dimension (increment or decrement)
        self.action_space = spaces.Discrete(2 * self.num_dims)

        # Start in the center of each dimension
        self.state = tuple(len(vals) // 2 for vals in self.weight_values)
        self.steps = 0
        self.max_steps = 500_000

        # Track best state for evaluation
        self.best_reward = float('-inf')
        self.best_state = self.state

    def run_simulation(self,weights):
        w_config = tuple(weights[:4])
        c_config = tuple(weights[4:])
        return lookup.get((w_config, c_config))


    def reset(self, seed=42, options=None):
        super().reset(seed=seed)
        # Random initialization for better exploration
        self.state = tuple(np.random.randint(0, len(vals)) for vals in self.weight_values)
        self.steps = 0
        return np.array(self.state, dtype=np.int32), {}

    def step(self, action):
        # Decode action: which dimension and which direction
        idxs = list(self.state)
        dim = action // 2
        increment = (action % 2) == 0

        # Update index within bounds
        if increment and idxs[dim] < len(self.weight_values[dim]) - 1:
            idxs[dim] += 1
        elif not increment and idxs[dim] > 0:
            idxs[dim] -= 1

        self.state = tuple(idxs)
        self.steps += 1

        # Map indices to actual weight values
        weights = [self.weight_values[i][idx] for i, idx in enumerate(self.state)]

        power, power1, error = self.run_simulation(weights)
        # New reward: 1/w1 + (w2)^2
        reward = float(-(error*1e6 + power1*1e6))

        # done = (self.steps >= self.max_steps)
        # return np.array(self.state, dtype=np.int32), reward, done, False, {}
        if reward > self.best_reward:
            self.best_reward = reward
            self.best_state = self.state

        done = self.steps >= self.max_steps

        # Add small Gaussian noise to promote exploration
        reward += np.random.normal(0, 0.1)

        return np.array(self.state, dtype=np.int32), reward, done, False, {}

    def render(self):
        weights = {f"w{i+1}": self.weight_values[i][idx]
                   for i, idx in enumerate(self.state)}
        state_str = ", ".join(f"{k}={v}" for k, v in weights.items())
        print(f"State: {state_str}")

        best_weights = {f"w{i+1}": self.weight_values[i][idx]
                        for i, idx in enumerate(self.best_state)}
        best_str = ", ".join(f"{k}={v}" for k, v in best_weights.items())
        print(f"Best state found: {best_str} (reward: {self.best_reward:.4f})")

